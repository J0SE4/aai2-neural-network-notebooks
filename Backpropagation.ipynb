{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9ujkdvlysKI"
      },
      "source": [
        "# Backpropagation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This task is about implementing the backpropagation algorithm.\n",
        "We will use the Stochastic Gradient Descent optimizer for optimizing the weights of a custom neural network.\n",
        "\n",
        "You can use numpy or torch for creating tensors, but not for the backpropagation (e.g. loss.backward() )!"
      ],
      "metadata": {
        "id": "b5GgeuKBy-_S"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCp9bVhDysKJ"
      },
      "source": [
        "## Prepare your dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MvflfmoSysKK"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import tqdm\n",
        "\n",
        "def load_mnist_data(root_path='./data', batch_size=4):\n",
        "    transform = transforms.Compose(\n",
        "        [transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5), (0.5))]\n",
        "    )\n",
        "\n",
        "    trainset = torchvision.datasets.MNIST(root=root_path, train=True, download=True, transform=transform)\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "    testset = torchvision.datasets.MNIST(root=root_path, train=False, download=True, transform=transform)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return trainloader, testloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PTQ7_cD2ysKK"
      },
      "source": [
        "## Building your neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "tSB556frysKK"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from typing import Any, Callable, Tuple\n",
        "\n",
        "##################################\n",
        "# For matrices or arbitrary size #\n",
        "##################################\n",
        "class MyWeightTensor:\n",
        "    def __init__(self, shape: Tuple or int, init_weight_fn: Callable = np.random.randn, init_weights: 'MyWeightTensor' or np.ndarray or int or float = None):\n",
        "        assert isinstance(shape, tuple) or isinstance(shape, int) or isinstance(shape, float), f'Allowed shapes: tuple, int, float, got: {type(shape)}'\n",
        "        self.shape = shape\n",
        "\n",
        "        if init_weights is not None:\n",
        "            if isinstance(init_weights, MyWeightTensor):\n",
        "                self.values = init_weights.values\n",
        "            else:\n",
        "                if isinstance(shape, tuple):\n",
        "                    assert isinstance(init_weights, np.ndarray)\n",
        "                else:\n",
        "                    assert isinstance(init_weights, int) or isinstance(init_weights, float)\n",
        "\n",
        "                self.values = init_weights\n",
        "        else:\n",
        "            if isinstance(shape, int):\n",
        "                self.shape = (self.shape,)\n",
        "                self.values = init_weight_fn(shape)\n",
        "            else:\n",
        "                self.values = init_weight_fn(*shape)\n",
        "\n",
        "    @property\n",
        "    def T(self) -> 'MyWeightTensor':\n",
        "        _T = self.values.T\n",
        "        return MyWeightTensor(shape=_T.shape, init_weights=_T)\n",
        "\n",
        "    def __add__(self, other) -> 'MyWeightTensor':\n",
        "        if isinstance(other, MyWeightTensor):\n",
        "            other = other.values\n",
        "        else:\n",
        "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
        "\n",
        "        return MyWeightTensor(shape=self.values.shape, init_weights=self.values + other)\n",
        "\n",
        "    def __mul__(self, other) -> 'MyWeightTensor':\n",
        "        if isinstance(other, MyWeightTensor):\n",
        "            other = other.values\n",
        "        else:\n",
        "            assert isinstance(other, np.ndarray) or isinstance(other, int) or isinstance(other, float)\n",
        "\n",
        "        _dot = np.dot(self.values, other)\n",
        "\n",
        "        return MyWeightTensor(shape=_dot.shape, init_weights=_dot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtKRvUzqysKL"
      },
      "source": [
        "For creating a linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "x4q7_4-EysKL"
      },
      "outputs": [],
      "source": [
        "class MyLinearLayer:\n",
        "    def __init__(self, in_features: int, out_features: int, init_weight_fn: Callable = np.random.randn) -> None:\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "\n",
        "        self.weights = MyWeightTensor(shape=(out_features, in_features), init_weight_fn=init_weight_fn)\n",
        "        self.bias = MyWeightTensor(shape=out_features, init_weight_fn=init_weight_fn)\n",
        "\n",
        "        self.latest_input = None\n",
        "        self.latest_output = None\n",
        "\n",
        "    def __call__(self, tensor: np.ndarray or MyWeightTensor) -> MyWeightTensor:\n",
        "        self.latest_input = tensor\n",
        "\n",
        "        bs = -1\n",
        "        if len(tensor.shape) == 2:\n",
        "            # batch size included\n",
        "            bs = tensor.shape[0]\n",
        "            _w = self.weights * tensor.T\n",
        "        else:\n",
        "            _w = self.weights * tensor\n",
        "\n",
        "        _bias = self.bias.values\n",
        "        if bs != -1:\n",
        "            _bias = np.tile(_bias, bs).reshape(bs, -1)\n",
        "\n",
        "        self.latest_output = (_w + _bias.T).T\n",
        "\n",
        "        return MyWeightTensor(shape=self.latest_output.shape, init_weights=self.latest_output)\n",
        "\n",
        "    def derivative(self) -> float:\n",
        "        assert self.latest_output is not None, 'Cannot calculate grad without a single forward pass.'\n",
        "        # Return a linear activation derivation\n",
        "        # your code\n",
        "        return 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eq6YvhTYysKM"
      },
      "source": [
        "Creating a custom neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "WxHQqGNQysKM"
      },
      "outputs": [],
      "source": [
        "def xavier_normal_init(*shape) -> np.ndarray:\n",
        "    assert len(shape) <= 2, 'Can only init max 2d tensors'\n",
        "    fan_in = shape[0]\n",
        "    if len(shape) == 1:\n",
        "        fan_out = fan_in\n",
        "    else:\n",
        "        fan_out = shape[1]\n",
        "    gain = 1.0\n",
        "\n",
        "    std = gain * np.sqrt(2.0 / (fan_in + fan_out))\n",
        "    return np.random.normal(loc=0.0, scale=std, size=shape)\n",
        "\n",
        "\n",
        "class MyNeuralNetwork:\n",
        "    def __init__(self) -> None:\n",
        "        # init_weight_fn = lambda *shape: np.random.randn(*shape) / 10\n",
        "        init_weight_fn = lambda *shape: xavier_normal_init(*shape)\n",
        "\n",
        "        # Build you own neural network with list of MyLinearLayer\n",
        "        # Note numbers of input and final output features of the neural network\n",
        "        # your code\n",
        "        self.layers = [\n",
        "            MyLinearLayer(784, 128, init_weight_fn=init_weight_fn),\n",
        "            MyLinearLayer(128, 64, init_weight_fn=init_weight_fn),\n",
        "            MyLinearLayer(64, 10, init_weight_fn=init_weight_fn)\n",
        "        ]\n",
        "\n",
        "    def __call__(self, tensor: np.ndarray) -> Any:\n",
        "        x = tensor\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHs-TF5XysKN"
      },
      "source": [
        "## Implement your loss function(s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U2nj4OObysKN"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "class CrossEntropyLoss:\n",
        "    def __init__(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def __call__(self, predictions: MyWeightTensor or np.ndarray, targets: MyWeightTensor or np.ndarray) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Computes cross entropy between targets snd predictions.\n",
        "        Returns: List of cross entropy losses (batch-wise)\n",
        "        \"\"\"\n",
        "        if isinstance(predictions, MyWeightTensor):\n",
        "            predictions = predictions.values\n",
        "\n",
        "        if isinstance(targets, MyWeightTensor):\n",
        "            targets = targets.values\n",
        "\n",
        "        assert predictions.shape[0] == targets.shape[0]\n",
        "        if len(targets.shape) == 2:\n",
        "            targets = targets.reshape(-1)\n",
        "        predictions = torch.as_tensor(predictions)\n",
        "        targets = torch.as_tensor(targets)\n",
        "\n",
        "        loss = np.array([F.cross_entropy(pred, t).item() for pred, t in zip(predictions, targets)])\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def derivative(self) -> Callable:\n",
        "        # y_hat is the prediction\n",
        "        # y is the target value\n",
        "        def _derivative(y_hat: MyWeightTensor or np.ndarray, y: MyWeightTensor or np.ndarray) -> np.ndarray:\n",
        "            if isinstance(y_hat, MyWeightTensor):\n",
        "                y_hat = y_hat.values\n",
        "\n",
        "            if isinstance(y, MyWeightTensor):\n",
        "                y = y.values\n",
        "\n",
        "            # implement the derivation of the cross entropy\n",
        "            # your code\n",
        "            exp_vals = np.exp(y_hat - np.max(y_hat, axis=1, keepdims=True))\n",
        "            softmax = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n",
        "            batch_size = y_hat.shape[0]\n",
        "            num_classes = y_hat.shape[1]\n",
        "            one_hot = np.zeros((batch_size, num_classes))\n",
        "            one_hot[np.arange(batch_size), y.flatten().astype(int)] = 1\n",
        "            return softmax - one_hot\n",
        "\n",
        "        return _derivative"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDeSsUdjysKN"
      },
      "source": [
        "## Implement the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tKnj943dysKN"
      },
      "outputs": [],
      "source": [
        "def train(model: MyNeuralNetwork, batch_size: int, learning_rate: float, loss_fn: Callable, epochs: int = 10):\n",
        "    train_loader, _ = load_mnist_data(batch_size=batch_size)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        running_accuracy = []\n",
        "        for imgs, targets in tqdm.tqdm(train_loader, desc=f'Training iteration {epoch + 1}'):\n",
        "\n",
        "            # for custom model\n",
        "            imgs = imgs.numpy()\n",
        "            targets = targets.numpy()\n",
        "\n",
        "            if len(targets.shape) == 1:\n",
        "                targets = targets.reshape(-1, 1)\n",
        "\n",
        "            imgs = imgs.reshape(-1, 28 * 28)\n",
        "\n",
        "            imgs = MyWeightTensor(shape=imgs.shape, init_weights=imgs)\n",
        "\n",
        "            outputs = model(imgs).values\n",
        "\n",
        "            loss = loss_fn(outputs, targets)\n",
        "            avg_loss = np.mean(loss)\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += avg_loss\n",
        "\n",
        "            # Calculate the Accuracy (how many of all samples are correctly classified?)\n",
        "            max_outputs = np.argmax(outputs, axis=1)\n",
        "            accuracy = (max_outputs == targets.flatten()).mean()\n",
        "            running_accuracy.append(accuracy)\n",
        "\n",
        "            #########################\n",
        "            # Start backpropagation #\n",
        "            #########################\n",
        "\n",
        "            # Your code for backpropagation!\n",
        "\n",
        "            # Step 1: starting from the very end with loss function\n",
        "            # beginning with gradients of the loss\n",
        "            loss_derivative = loss_fn.derivative()\n",
        "            d_loss = loss_derivative(outputs, targets)\n",
        "\n",
        "            # Step 2: start back propagating the neural network\n",
        "            # for each layer calculate the output gradient and update weights and bias\n",
        "            d_out = d_loss\n",
        "            for i in range(len(model.layers) - 1, -1, -1):\n",
        "                layer = model.layers[i]\n",
        "                if isinstance(layer.latest_input, MyWeightTensor):\n",
        "                    layer_input = layer.latest_input.values\n",
        "                else:\n",
        "                    layer_input = layer.latest_input\n",
        "                d_weights = np.dot(d_out.T, layer_input)\n",
        "                d_bias = np.sum(d_out, axis=0)\n",
        "                d_input = np.dot(d_out, layer.weights.values)\n",
        "                layer.weights.values -= learning_rate * d_weights\n",
        "                layer.bias.values -= learning_rate * d_bias\n",
        "                d_out = d_input * layer.derivative()\n",
        "\n",
        "            #######################\n",
        "            # End backpropagation #\n",
        "            #######################\n",
        "\n",
        "        print(f'Epoch {epoch + 1} finished with loss: {running_loss / len(train_loader):.3f} and accuracy: {torch.tensor(running_accuracy).mean():.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2HZUDFegysKO",
        "outputId": "c1201be6-6a55-4f24-ba77-83d3a2272159"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:01<00:00, 5.00MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 131kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:01<00:00, 1.24MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 8.95MB/s]\n",
            "Training iteration 1: 100%|██████████| 15000/15000 [00:53<00:00, 280.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 finished with loss: 0.402 and accuracy: 0.880\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training iteration 2: 100%|██████████| 15000/15000 [00:54<00:00, 276.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 finished with loss: 0.330 and accuracy: 0.905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training iteration 3: 100%|██████████| 15000/15000 [00:54<00:00, 273.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 finished with loss: 0.316 and accuracy: 0.910\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training iteration 4: 100%|██████████| 15000/15000 [00:53<00:00, 279.98it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 finished with loss: 0.308 and accuracy: 0.911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training iteration 5: 100%|██████████| 15000/15000 [00:53<00:00, 278.07it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 finished with loss: 0.302 and accuracy: 0.914\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training iteration 6: 100%|██████████| 15000/15000 [00:53<00:00, 279.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 finished with loss: 0.298 and accuracy: 0.916\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training iteration 7: 100%|██████████| 15000/15000 [00:54<00:00, 274.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 finished with loss: 0.293 and accuracy: 0.917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training iteration 8: 100%|██████████| 15000/15000 [00:54<00:00, 277.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 finished with loss: 0.292 and accuracy: 0.917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training iteration 9: 100%|██████████| 15000/15000 [00:54<00:00, 277.40it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 finished with loss: 0.289 and accuracy: 0.918\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training iteration 10: 100%|██████████| 15000/15000 [00:54<00:00, 274.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 finished with loss: 0.287 and accuracy: 0.917\n"
          ]
        }
      ],
      "source": [
        "#############################\n",
        "# Execute the training loop #\n",
        "#############################\n",
        "model = MyNeuralNetwork()\n",
        "batch_size = 4\n",
        "learning_rate = 0.001\n",
        "epochs = 10\n",
        "loss_fn = CrossEntropyLoss()\n",
        "\n",
        "train(\n",
        "    model=model,\n",
        "    batch_size=batch_size,\n",
        "    learning_rate=learning_rate,\n",
        "    epochs=epochs,\n",
        "    loss_fn=loss_fn\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "25301cabe4c6f833fd20f15b1b22933971919908771eb627a83fe325b4fb6671"
      }
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}